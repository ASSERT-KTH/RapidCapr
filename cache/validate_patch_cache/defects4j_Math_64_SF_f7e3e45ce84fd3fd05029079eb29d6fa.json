{
    "patch": "        final double initCost = cost;\n        if (checker != null) {\n            checker.addIteration(new VectorialPointValuePair(point, objective));\n        }\n\n        // outer loop\n        lmPar = 0;\n        boolean firstIteration = true;\n        VectorialPointValuePair current = new VectorialPointValuePair(point, objective);\n        while (true) {\n            incrementIterationsCounter();\n\n            // compute the Q.R. decomposition of the jacobian matrix\n            VectorialPointValuePair previous = current;\n            updateJacobian();\n            qrDecomposition();\n\n            // compute Qt.res\n            qTy(residuals);\n            // now we don't need Q anymore,\n            // so let jacobian contain the R matrix with its diagonal elements\n            for (int k = 0; k < solvedCols; ++k) {\n                int pk = permutation[k];\n                jacobian[k][pk] = diagR[pk];\n            }\n\n            if (firstIteration) {\n\n                // scale the point according to the norms of the columns\n                // of the initial jacobian\n                xNorm = 0;\n                for (int k = 0; k < cols; ++k) {\n                    double dk = jacNorm[k];\n                    if (dk == 0) {\n                        dk = 1.0;\n                    }\n                    double xk = dk * point[k];\n                    xNorm  += xk * xk;\n                    diag[k] = dk;\n                }\n                xNorm = Math.sqrt(xNorm);\n\n                // initialize the step bound delta\n                delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n\n            }\n\n            // check orthogonality between function vector and jacobian columns\n            double maxCosine = 0;\n            if (cost != 0) {\n                for (int j = 0; j < solvedCols; ++j) {\n                    int    pj = permutation[j];\n                    double s  = jacNorm[pj];\n                    if (s != 0) {\n                        double sum = 0;\n                        for (int i = 0; i <= j; ++i) {\n                            sum += jacobian[i][pj] * residuals[i];\n                        }\n                        maxCosine = Math.max(maxCosine, Math.abs(sum) / (s * cost));\n                    }\n                }\n            }\n            if (maxCosine <= orthoTolerance) {\n                // convergence has been reached\n                return current;\n            }\n\n            // rescale if necessary\n            for (int j = 0; j < cols; ++j) {\n                diag[j] = Math.max(diag[j], jacNorm[j]);\n            }\n\n            // inner loop\n            for (double ratio = 0; ratio < 1.0e-4;) {\n\n                // save the state\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    oldX[pj] = point[pj];\n                }\n                double previousCost = cost;\n                double[] tmpVec = residuals;\n                residuals = oldRes;\n                oldRes    = tmpVec;\n\n                // determine the Levenberg-Marquardt parameter\n                determineLMParameter(oldRes, delta, diag, work1, work2, work3);\n\n                // compute the new point and the norm of the evolution direction\n                double lmNorm = 0;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    lmDir[pj] = -lmDir[pj];\n                    point[pj] = oldX[pj] + lmDir[pj];\n                    double s = diag[pj] * lmDir[pj];\n                    lmNorm  += s * s;\n                }\n                lmNorm = Math.sqrt(lmNorm);\n                // on the first iteration, adjust the initial step bound.\n                if (firstIteration) {\n                    delta = Math.min(delta, lmNorm);\n                }\n\n                // evaluate the function at x + p and calculate its norm\n                updateResidualsAndCost();\n                current = new VectorialPointValuePair(point, objective);\n                if (checker != null) {\n                    checker.addIteration(current);\n                }\n\n                // compute the scaled actual reduction\n                double actRed;\n                if (rho < 0) {\n                    // Brooks: rho == -1.0: usually this would indicate convergence,\n                    // but if the rank of the Jacobian is deficient or ill-conditioned,\n                    // then arbitrarily choose an initial value of lmPar and continue.\n                    // Brooks chose a value of .001.\n                    lmPar = sumCols(1, solvedCols, par) / solvedCols;\n                    return previous;\n                }\n                actRed = -1.0;\n                if (rho >= p0001) {\n                    // The step is OK, compute the actual reduction.\n                    // The lower bound on the reduction can be based on the\n                    // old X - current X difference on bound steps, since all\n                    // bound steps use a step of exactly boundStep.\n                    int iBound = 0, nBound = boundCols.length;\n                    double sbMin = Double.NaN;\n                    for (int k = 0; k < solvedCols; ++k) {\n                        int pk = permutation[k];\n                        if (k == boundCols[iBound]) {\n                            double ratio_ = boundStep / diag[pk];\n                            ++iBound;\n                            if (Double.isNaN(sbMin)) {\n                                sbMin = ratio_;\n                            } else {\n                                sbMin = Math.min(sbMin, ratio_);\n                            }\n                        }\n                    } // for (solvedCols)\n\n                    final double preredMag = (p5 * rho * rho * lmNormSq +\n                                              lmPar * lmNormSq);\n                    // Form the ratio of the actual to predicted\n                    // reduction and check it against the tolerance.\n                    // If the step is too big, then lmPar is increased\n                    // to bring the step size down to the tolerance.\n                    if (sbMin < 1.0) {\n                        actRed = preredMag * sbMin * sbMin;\n                    } else {\n                        actRed = preredMag;\n                    }\n                    if (actRed < costRelativeTolerance) {\n                        return current;\n                    }\n\n                }\n\n                // Compute the scaled predicted reduction.\n                //\n                // If it is small enough (depending on both the size of the\n                // actual reduction and lmPar) then accept the current step.\n                final double preRed;\n                if (Double.isNaN(actRed)) {\n                    preRed = Double.NaN;\n                } else {\n                    final double f = 1 - lmPar * p5 * rho;\n                    preRed = f * f * f * rho * lmNormSq + lmPar * rho * betaSum;\n                    if (preRed < 0) {\n                        preRed = Double.NaN;\n                    } else {\n                        if (rho > p001) {\n                            // Update lmPar for next iteration.\n                            lmPar = Math.min(lmPar, p4);\n                        } else {\n                            lmPar *= 5;\n                        }\n                    }\n                }\n\n                // ratio of the actual to the predicted reduction\n                ratio = (preRed == null || preRed == Double.NaN) ? 2 * costRelativeTolerance : (actRed / preRed);\n                // Update jacobian and QR decomposition.\n                updateJacobian();\n                qrDecomposition();\n\n                // compute Qt.res\n                qTy(residuals);\n                // now we don't need Q anymore\n                // so let jacobian contain the R matrix with its diagonal elements\n                for (int k = 0; k < solvedCols; ++k) {\n                    int pk = permutation[k];\n                    jacobian[k][pk] = diagR[pk];\n                }\n\n                // rescale if necessary\n                for (int j = 0; j < cols; ++j) {\n                    diag[j] = Math.max(diag[j], jacNorm[j]);\n                }\n\n                // Test for successful iteration.\n                if (ratio >= p0001) {\n\n                    // Successful iteration\n                    firstIteration = false;\n                    xNorm = 0;\n                    for (int k = 0; k < cols; +",
    "patch_diff": "diff --git a/src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java b/src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java\nindex 50914932e..60d30679f 100644\n--- a/src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java\n+++ b/src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java\n@@ -238,29 +238,10 @@ public class LevenbergMarquardtOptimizer extends AbstractLeastSquaresOptimizer {\n \n     /** {@inheritDoc} */\n     @Override\n-    protected VectorialPointValuePair doOptimize()\n-        throws FunctionEvaluationException, OptimizationException, IllegalArgumentException {\n-\n-        // arrays shared with the other private methods\n-        solvedCols  = Math.min(rows, cols);\n-        diagR       = new double[cols];\n-        jacNorm     = new double[cols];\n-        beta        = new double[cols];\n-        permutation = new int[cols];\n-        lmDir       = new double[cols];\n-\n-        // local point\n-        double   delta   = 0;\n-        double   xNorm   = 0;\n-        double[] diag    = new double[cols];\n-        double[] oldX    = new double[cols];\n-        double[] oldRes  = new double[rows];\n-        double[] work1   = new double[cols];\n-        double[] work2   = new double[cols];\n-        double[] work3   = new double[cols];\n-\n-        // evaluate the function at the starting point and calculate its norm\n-        updateResidualsAndCost();\n+        final double initCost = cost;\n+        if (checker != null) {\n+            checker.addIteration(new VectorialPointValuePair(point, objective));\n+        }\n \n         // outer loop\n         lmPar = 0;\n@@ -363,105 +344,107 @@ public class LevenbergMarquardtOptimizer extends AbstractLeastSquaresOptimizer {\n                 // evaluate the function at x + p and calculate its norm\n                 updateResidualsAndCost();\n                 current = new VectorialPointValuePair(point, objective);\n-\n-                // compute the scaled actual reduction\n-                double actRed = -1.0;\n-                if (0.1 * cost < previousCost) {\n-                    double r = cost / previousCost;\n-                    actRed = 1.0 - r * r;\n+                if (checker != null) {\n+                    checker.addIteration(current);\n                 }\n \n-                // compute the scaled predicted reduction\n-                // and the scaled directional derivative\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    double dirJ = lmDir[pj];\n-                    work1[j] = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        work1[i] += jacobian[i][pj] * dirJ;\n-                    }\n-                }\n-                double coeff1 = 0;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    coeff1 += work1[j] * work1[j];\n+                // compute the scaled actual reduction\n+                double actRed;\n+                if (rho < 0) {\n+                    // Brooks: rho == -1.0: usually this would indicate convergence,\n+                    // but if the rank of the Jacobian is deficient or ill-conditioned,\n+                    // then arbitrarily choose an initial value of lmPar and continue.\n+                    // Brooks chose a value of .001.\n+                    lmPar = sumCols(1, solvedCols, par) / solvedCols;\n+                    return previous;\n                 }\n-                double pc2 = previousCost * previousCost;\n-                coeff1 = coeff1 / pc2;\n-                double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-                double preRed = coeff1 + 2 * coeff2;\n-                double dirDer = -(coeff1 + coeff2);\n-\n-                // ratio of the actual to the predicted reduction\n-                ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-\n-                // update the step bound\n-                if (ratio <= 0.25) {\n-                    double tmp =\n-                        (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                        if ((0.1 * cost >= previousCost) || (tmp < 0.1)) {\n-                            tmp = 0.1;\n+                actRed = -1.0;\n+                if (rho >= p0001) {\n+                    // The step is OK, compute the actual reduction.\n+                    // The lower bound on the reduction can be based on the\n+                    // old X - current X difference on bound steps, since all\n+                    // bound steps use a step of exactly boundStep.\n+                    int iBound = 0, nBound = boundCols.length;\n+                    double sbMin = Double.NaN;\n+                    for (int k = 0; k < solvedCols; ++k) {\n+                        int pk = permutation[k];\n+                        if (k == boundCols[iBound]) {\n+                            double ratio_ = boundStep / diag[pk];\n+                            ++iBound;\n+                            if (Double.isNaN(sbMin)) {\n+                                sbMin = ratio_;\n+                            } else {\n+                                sbMin = Math.min(sbMin, ratio_);\n+                            }\n                         }\n-                        delta = tmp * Math.min(delta, 10.0 * lmNorm);\n-                        lmPar /= tmp;\n-                } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                    delta = 2 * lmNorm;\n-                    lmPar *= 0.5;\n-                }\n-\n-                // test for successful iteration.\n-                if (ratio >= 1.0e-4) {\n-                    // successful iteration, update the norm\n-                    firstIteration = false;\n-                    xNorm = 0;\n-                    for (int k = 0; k < cols; ++k) {\n-                        double xK = diag[k] * point[k];\n-                        xNorm    += xK * xK;\n+                    } // for (solvedCols)\n+\n+                    final double preredMag = (p5 * rho * rho * lmNormSq +\n+                                              lmPar * lmNormSq);\n+                    // Form the ratio of the actual to predicted\n+                    // reduction and check it against the tolerance.\n+                    // If the step is too big, then lmPar is increased\n+                    // to bring the step size down to the tolerance.\n+                    if (sbMin < 1.0) {\n+                        actRed = preredMag * sbMin * sbMin;\n+                    } else {\n+                        actRed = preredMag;\n                     }\n-                    xNorm = Math.sqrt(xNorm);\n-\n-                    // tests for convergence.\n-                    // we use the vectorial convergence checker\n-                } else {\n-                    // failed iteration, reset the previous values\n-                    cost = previousCost;\n-                    for (int j = 0; j < solvedCols; ++j) {\n-                        int pj = permutation[j];\n-                        point[pj] = oldX[pj];\n+                    if (actRed < costRelativeTolerance) {\n+                        return current;\n                     }\n-                    tmpVec    = residuals;\n-                    residuals = oldRes;\n-                    oldRes    = tmpVec;\n+\n                 }\n-                if (checker==null) {\n-                \tif (((Math.abs(actRed) <= costRelativeTolerance) &&\n-                        (preRed <= costRelativeTolerance) &&\n-                        (ratio <= 2.0)) ||\n-                       (delta <= parRelativeTolerance * xNorm)) {\n-                       return current;\n-                   }\n+\n+                // Compute the scaled predicted reduction.\n+                //\n+                // If it is small enough (depending on both the size of the\n+                // actual reduction and lmPar) then accept the current step.\n+                final double preRed;\n+                if (Double.isNaN(actRed)) {\n+                    preRed = Double.NaN;\n                 } else {\n-                    if (checker.converged(getIterations(), previous, current)) {\n-                        return current;\n+                    final double f = 1 - lmPar * p5 * rho;\n+                    preRed = f * f * f * rho * lmNormSq + lmPar * rho * betaSum;\n+                    if (preRed < 0) {\n+                        preRed = Double.NaN;\n+                    } else {\n+                        if (rho > p001) {\n+                            // Update lmPar for next iteration.\n+                            lmPar = Math.min(lmPar, p4);\n+                        } else {\n+                            lmPar *= 5;\n+                        }\n                     }\n                 }\n-                // tests for termination and stringent tolerances\n-                // (2.2204e-16 is the machine epsilon for IEEE754)\n-                if ((Math.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE,\n-                            costRelativeTolerance);\n-                } else if (delta <= 2.2204e-16 * xNorm) {\n-                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE,\n-                            parRelativeTolerance);\n-                } else if (maxCosine <= 2.2204e-16)  {\n-                    throw new OptimizationException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE,\n-                            orthoTolerance);\n+\n+                // ratio of the actual to the predicted reduction\n+                ratio = (preRed == null || preRed == Double.NaN) ? 2 * costRelativeTolerance : (actRed / preRed);\n+                // Update jacobian and QR decomposition.\n+                updateJacobian();\n+                qrDecomposition();\n+\n+                // compute Qt.res\n+                qTy(residuals);\n+                // now we don't need Q anymore\n+                // so let jacobian contain the R matrix with its diagonal elements\n+                for (int k = 0; k < solvedCols; ++k) {\n+                    int pk = permutation[k];\n+                    jacobian[k][pk] = diagR[pk];\n                 }\n \n-            }\n+                // rescale if necessary\n+                for (int j = 0; j < cols; ++j) {\n+                    diag[j] = Math.max(diag[j], jacNorm[j]);\n+                }\n \n-        }\n+                // Test for successful iteration.\n+                if (ratio >= p0001) {\n \n-    }\n+                    // Successful iteration\n+                    firstIteration = false;\n+                    xNorm = 0;\n+                    for (int k = 0; k < cols; +\n \n     /**\n      * Determine the Levenberg-Marquardt parameter.",
    "result_reason": "error: illegal start of type",
    "test_result": "ERROR"
}